{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from itertools import chain\nfrom typing import List, Tuple, Union\n\nimport pandas as pd\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.normalizers import Lowercase\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nimport random\nimport re\nfrom collections import Counter\n\nimport gensim.downloader as api\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom IPython.display import clear_output\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim import Adam\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm","metadata":{"id":"HzpDGi60ODzD","execution":{"iopub.status.busy":"2023-04-30T18:42:50.304493Z","iopub.execute_input":"2023-04-30T18:42:50.305120Z","iopub.status.idle":"2023-04-30T18:42:50.445959Z","shell.execute_reply.started":"2023-04-30T18:42:50.305080Z","shell.execute_reply":"2023-04-30T18:42:50.444890Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.read_parquet('/kaggle/input/nuclear-vk-bots/train.parquet')\ntest = pd.read_parquet('/kaggle/input/nuclear-vk-bots/test.parquet')","metadata":{"id":"n8sl7UX7ODzF","execution":{"iopub.status.busy":"2023-04-30T18:42:51.031476Z","iopub.execute_input":"2023-04-30T18:42:51.032812Z","iopub.status.idle":"2023-04-30T18:42:51.616431Z","shell.execute_reply.started":"2023-04-30T18:42:51.032762Z","shell.execute_reply":"2023-04-30T18:42:51.615187Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def clear(txt):\n  txt = str(txt)\n  txt = txt[1:]\n  txt = txt.replace(\"'\", \"\")\n  txt = txt.replace(\"[\", \"\")\n  txt = txt.replace(\"]\", \"\")\n  txt = txt.replace(\",\", \" \")\n  txt = txt.replace('\"', '')\n  txt = txt.replace(\"\\n\", \"\")\n  return txt","metadata":{"id":"88PpQLCBPfeE","execution":{"iopub.status.busy":"2023-04-30T18:42:51.618909Z","iopub.execute_input":"2023-04-30T18:42:51.619636Z","iopub.status.idle":"2023-04-30T18:42:51.628037Z","shell.execute_reply.started":"2023-04-30T18:42:51.619590Z","shell.execute_reply":"2023-04-30T18:42:51.626726Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train['ciphers'] = train['ciphers'].apply(clear)\ntest['ciphers'] = test['ciphers'].apply(clear)\ntrain['curves'] = train['ciphers'].apply(clear)\ntest['curves'] = test['curves'].apply(clear)","metadata":{"id":"pk_qynUaPoLe","execution":{"iopub.status.busy":"2023-04-30T18:42:51.787173Z","iopub.execute_input":"2023-04-30T18:42:51.788209Z","iopub.status.idle":"2023-04-30T18:42:57.975904Z","shell.execute_reply.started":"2023-04-30T18:42:51.788159Z","shell.execute_reply":"2023-04-30T18:42:57.974639Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train['text'] = train.apply(lambda row: f\"{row.ciphers} [SEP] {row.curves} [SEP] {row.ua}\", axis=1)","metadata":{"id":"5eotQnPFODzF","execution":{"iopub.status.busy":"2023-04-30T18:42:57.978580Z","iopub.execute_input":"2023-04-30T18:42:57.979474Z","iopub.status.idle":"2023-04-30T18:42:59.706385Z","shell.execute_reply.started":"2023-04-30T18:42:57.979432Z","shell.execute_reply":"2023-04-30T18:42:59.705237Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test['text'] = test.apply(lambda row: f\"{row.ciphers} [SEP] {row.curves} [SEP] {row.ua}\", axis=1)","metadata":{"id":"jpM4iCfOODzG","execution":{"iopub.status.busy":"2023-04-30T18:42:59.708245Z","iopub.execute_input":"2023-04-30T18:42:59.708702Z","iopub.status.idle":"2023-04-30T18:43:00.124700Z","shell.execute_reply.started":"2023-04-30T18:42:59.708653Z","shell.execute_reply":"2023-04-30T18:43:00.123302Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_new = train.drop([\"ciphers\", \"curves\", \"ua\"], axis = 1)\n\ntest_new = test.drop([\"ciphers\", \"curves\", \"ua\"], axis = 1)","metadata":{"id":"RGx8QWpKODzG","execution":{"iopub.status.busy":"2023-04-30T18:43:00.127754Z","iopub.execute_input":"2023-04-30T18:43:00.128190Z","iopub.status.idle":"2023-04-30T18:43:00.149317Z","shell.execute_reply.started":"2023-04-30T18:43:00.128129Z","shell.execute_reply":"2023-04-30T18:43:00.148161Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification # https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification\nfrom transformers import RobertaModel  # https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaForSequenceClassification\nfrom transformers import DistilBertModel\nfrom transformers import GPT2Model\nfrom transformers import BertModel","metadata":{"id":"gv1V3HsqODzH","execution":{"iopub.status.busy":"2023-04-30T18:43:00.152072Z","iopub.execute_input":"2023-04-30T18:43:00.152790Z","iopub.status.idle":"2023-04-30T18:43:00.176380Z","shell.execute_reply.started":"2023-04-30T18:43:00.152751Z","shell.execute_reply":"2023-04-30T18:43:00.175423Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nold_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\nnew_tokenizer = old_tokenizer.train_new_from_iterator([f\"{row.ciphers} [SEP] {row.curves} [SEP] {row.ua}\" for row in chain(train.itertuples(), test.itertuples())], \n                                                   vocab_size= 15000)\nnew_tokenizer.save_pretrained(\"/kaggle/working/bruhwalkk\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:43:00.178116Z","iopub.execute_input":"2023-04-30T18:43:00.178578Z","iopub.status.idle":"2023-04-30T18:43:18.042518Z","shell.execute_reply.started":"2023-04-30T18:43:00.178535Z","shell.execute_reply":"2023-04-30T18:43:18.041447Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6412734e4a944001a2823b77248f8216"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d05ec19e5964872a15b794c0d31e2c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fb2158873954ccc984bdab03a0ead04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf2e0b389cf4589884398a409080924"}},"metadata":{}},{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/bruhwalkk/tokenizer_config.json',\n '/kaggle/working/bruhwalkk/special_tokens_map.json',\n '/kaggle/working/bruhwalkk/vocab.txt',\n '/kaggle/working/bruhwalkk/added_tokens.json',\n '/kaggle/working/bruhwalkk/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer\n\nnew_tokenizer = DistilBertTokenizer.from_pretrained(\"/kaggle/working/bruhwalkk\")\nnew_tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:43:18.044272Z","iopub.execute_input":"2023-04-30T18:43:18.044651Z","iopub.status.idle":"2023-04-30T18:43:18.070329Z","shell.execute_reply.started":"2023-04-30T18:43:18.044613Z","shell.execute_reply":"2023-04-30T18:43:18.069189Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DistilBertTokenizer(name_or_path='/kaggle/working/bruhwalkk', vocab_size=10996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_data = Dataset.from_pandas(train_new)\ntest_data = Dataset.from_pandas(test_new)","metadata":{"id":"5CI0fS6uODzJ","execution":{"iopub.status.busy":"2023-04-30T18:43:18.073714Z","iopub.execute_input":"2023-04-30T18:43:18.074063Z","iopub.status.idle":"2023-04-30T18:43:18.165536Z","shell.execute_reply.started":"2023-04-30T18:43:18.074031Z","shell.execute_reply":"2023-04-30T18:43:18.164354Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def tokenization(example):\n    return new_tokenizer.batch_encode_plus(example['text'], add_special_tokens=True, return_token_type_ids=False, truncation=True)\n\ntrain_dataset = train_data.map(tokenization, batched=True)\ntest_dataset = test_data.map(tokenization, batched=True)","metadata":{"id":"A6U7Y8cCODzJ","outputId":"6ebeafbe-3d80-4dbe-cecc-233f3591088b","scrolled":true,"execution":{"iopub.status.busy":"2023-04-30T18:43:18.167268Z","iopub.execute_input":"2023-04-30T18:43:18.167684Z","iopub.status.idle":"2023-04-30T18:47:18.741695Z","shell.execute_reply.started":"2023-04-30T18:43:18.167643Z","shell.execute_reply":"2023-04-30T18:47:18.740254Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cacaa89eb384070a8e4129c488c2f79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66fa51e9a11a42aa95aff73f6701ad3b"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset.set_format(type=\"torch\", columns=[\"id\", \"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(type=\"torch\", columns=[\"id\", \"input_ids\", \"attention_mask\"])","metadata":{"id":"qn0DKZGZODzK","execution":{"iopub.status.busy":"2023-04-30T18:47:18.747309Z","iopub.execute_input":"2023-04-30T18:47:18.748024Z","iopub.status.idle":"2023-04-30T18:47:18.755813Z","shell.execute_reply.started":"2023-04-30T18:47:18.747964Z","shell.execute_reply":"2023-04-30T18:47:18.754643Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=new_tokenizer)","metadata":{"id":"SEtItbHGODzL","execution":{"iopub.status.busy":"2023-04-30T18:47:18.757614Z","iopub.execute_input":"2023-04-30T18:47:18.758083Z","iopub.status.idle":"2023-04-30T18:47:18.774399Z","shell.execute_reply.started":"2023-04-30T18:47:18.758032Z","shell.execute_reply":"2023-04-30T18:47:18.773168Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, collate_fn=data_collator, pin_memory=True, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, collate_fn=data_collator, pin_memory=True, shuffle=False)","metadata":{"id":"wdaevMN2ODzL","execution":{"iopub.status.busy":"2023-04-30T18:47:18.776033Z","iopub.execute_input":"2023-04-30T18:47:18.776488Z","iopub.status.idle":"2023-04-30T18:47:18.786431Z","shell.execute_reply.started":"2023-04-30T18:47:18.776430Z","shell.execute_reply":"2023-04-30T18:47:18.785379Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertConfig\n\nconfig = DistilBertConfig(\n    vocab_size=new_tokenizer.vocab_size,  # we align this to the tokenizer vocab_size\n    max_position_embeddings=514,\n    hidden_size=768,\n    num_attention_heads=12,\n    num_hidden_layers=6,\n    type_vocab_size=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:47:18.790018Z","iopub.execute_input":"2023-04-30T18:47:18.790326Z","iopub.status.idle":"2023-04-30T18:47:18.797234Z","shell.execute_reply.started":"2023-04-30T18:47:18.790299Z","shell.execute_reply":"2023-04-30T18:47:18.796218Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification\n\nmod = DistilBertForSequenceClassification(config)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:47:18.799078Z","iopub.execute_input":"2023-04-30T18:47:18.799630Z","iopub.status.idle":"2023-04-30T18:47:19.638676Z","shell.execute_reply.started":"2023-04-30T18:47:18.799594Z","shell.execute_reply":"2023-04-30T18:47:19.637497Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\nprint(torch.cuda.get_device_name())","metadata":{"id":"x7Qi-m-7ODzL","execution":{"iopub.status.busy":"2023-04-30T18:47:19.640195Z","iopub.execute_input":"2023-04-30T18:47:19.641221Z","iopub.status.idle":"2023-04-30T18:47:19.733934Z","shell.execute_reply.started":"2023-04-30T18:47:19.641179Z","shell.execute_reply":"2023-04-30T18:47:19.732702Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"cuda:0\nTesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:47:55.444979Z","iopub.execute_input":"2023-04-30T18:47:55.445699Z","iopub.status.idle":"2023-04-30T18:47:55.471238Z","shell.execute_reply.started":"2023-04-30T18:47:55.445654Z","shell.execute_reply":"2023-04-30T18:47:55.470061Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3bcd32a8cb2421bb52d6ffd99fa8c23"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification\n\nmod2 = DistilBertForSequenceClassification.from_pretrained(\"bruhwalkk/distilbert-base-uncased-finetuned-vk\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:48:32.087370Z","iopub.execute_input":"2023-04-30T18:48:32.088605Z","iopub.status.idle":"2023-04-30T18:48:33.071612Z","shell.execute_reply.started":"2023-04-30T18:48:32.088560Z","shell.execute_reply":"2023-04-30T18:48:33.070521Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bruhwalkk/distilbert-base-uncased-finetuned-vk were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bruhwalkk/distilbert-base-uncased-finetuned-vk and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nmod2 = mod2.to(device)\n\ntraining_args = TrainingArguments(\n    save_steps = 5000,\n    logging_strategy = \"epoch\",\n    report_to=\"none\",\n    output_dir=\"distilbert-base-uncased-finetuned-vk-classification_last\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=6,\n    push_to_hub = True,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=mod2,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=new_tokenizer,\n    data_collator=data_collator\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:48:36.905496Z","iopub.execute_input":"2023-04-30T18:48:36.906582Z","iopub.status.idle":"2023-04-30T20:45:28.768820Z","shell.execute_reply.started":"2023-04-30T18:48:36.906521Z","shell.execute_reply":"2023-04-30T20:45:28.767303Z"},"scrolled":true,"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/distilbert-base-uncased-finetuned-vk-classification_last is already a clone of https://huggingface.co/bruhwalkk/distilbert-base-uncased-finetuned-vk-classification_last. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='17982' max='17982' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [17982/17982 1:56:48, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2997</td>\n      <td>0.360700</td>\n    </tr>\n    <tr>\n      <td>5994</td>\n      <td>0.319600</td>\n    </tr>\n    <tr>\n      <td>8991</td>\n      <td>0.294800</td>\n    </tr>\n    <tr>\n      <td>11988</td>\n      <td>0.278800</td>\n    </tr>\n    <tr>\n      <td>14985</td>\n      <td>0.262300</td>\n    </tr>\n    <tr>\n      <td>17982</td>\n      <td>0.249600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=17982, training_loss=0.29431081743635995, metrics={'train_runtime': 7008.4914, 'train_samples_per_second': 41.048, 'train_steps_per_second': 2.566, 'total_flos': 3.4377237894678252e+16, 'train_loss': 0.29431081743635995, 'epoch': 6.0})"},"metadata":{}}]},{"cell_type":"code","source":"pred, _ , _ = trainer.predict(test_dataset)\npred = torch.sigmoid(torch.tensor(pred))","metadata":{"execution":{"iopub.status.busy":"2023-04-30T20:45:28.771179Z","iopub.execute_input":"2023-04-30T20:45:28.773358Z","iopub.status.idle":"2023-04-30T20:46:53.235665Z","shell.execute_reply.started":"2023-04-30T20:45:28.773291Z","shell.execute_reply":"2023-04-30T20:46:53.234617Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"test_res = pd.DataFrame({\n        \"id\": test_dataset[\"id\"].numpy(),\n        \"is_bot\": pred[:, 1].numpy()})","metadata":{"execution":{"iopub.status.busy":"2023-04-30T20:46:53.237164Z","iopub.execute_input":"2023-04-30T20:46:53.237831Z","iopub.status.idle":"2023-04-30T20:46:53.250900Z","shell.execute_reply.started":"2023-04-30T20:46:53.237801Z","shell.execute_reply":"2023-04-30T20:46:53.249741Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"test_res.to_csv(\"submission_bert_moment-tuned-best?.csv\", index=None)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T20:47:45.702071Z","iopub.execute_input":"2023-04-30T20:47:45.703135Z","iopub.status.idle":"2023-04-30T20:47:45.732657Z","shell.execute_reply.started":"2023-04-30T20:47:45.703072Z","shell.execute_reply":"2023-04-30T20:47:45.731639Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"pred2, _, _ = trainer.predict(train_dataset)\npred2 = torch.sigmoid(torch.tensor(pred2))","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:47:53.155458Z","iopub.status.idle":"2023-04-30T18:47:53.156365Z","shell.execute_reply.started":"2023-04-30T18:47:53.156065Z","shell.execute_reply":"2023-04-30T18:47:53.156098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_res = pd.DataFrame({\n        \"id\": train_dataset[\"id\"].numpy(),\n        \"is_bot\": pred2[:, 1].numpy(), \n    \"label\": train_dataset[\"label\"].numpy()})","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:47:53.158453Z","iopub.status.idle":"2023-04-30T18:47:53.159026Z","shell.execute_reply.started":"2023-04-30T18:47:53.158753Z","shell.execute_reply":"2023-04-30T18:47:53.158781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint(f\"ROC-AUC SCORE FOR TRAIN IS: {roc_auc_score(train_res[\"label\"], train_res[\"is_bot\"])}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:47:53.160906Z","iopub.status.idle":"2023-04-30T18:47:53.161448Z","shell.execute_reply.started":"2023-04-30T18:47:53.161156Z","shell.execute_reply":"2023-04-30T18:47:53.161181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(\"distilbert-base-uncased-finetuned-vk-classification_last\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:47:53.163423Z","iopub.status.idle":"2023-04-30T18:47:53.163907Z","shell.execute_reply.started":"2023-04-30T18:47:53.163658Z","shell.execute_reply":"2023-04-30T18:47:53.163683Z"},"trusted":true},"execution_count":null,"outputs":[]}]}